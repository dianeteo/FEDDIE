{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a47653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\teomi\\Projects\\FEDDIE\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using Selenium for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Using transformers to load models\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfc9b7",
   "metadata": {},
   "source": [
    "## Loading most recent statements/transcripts/minutes/articles from websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4003a1",
   "metadata": {},
   "source": [
    "FOMC meeting minutes/press conference transcripts/statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e6119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Latest meeting selected: 2025-06-17\n",
      "✅ Saved all documents to ./fomc.json\n"
     ]
    }
   ],
   "source": [
    "# === Setup headless Selenium\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(\"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\")\n",
    "time.sleep(2)\n",
    "\n",
    "# === Grab all meeting blocks\n",
    "meeting_blocks = driver.find_elements(By.CSS_SELECTOR, \".fomc-meeting\")\n",
    "today = datetime.today()\n",
    "\n",
    "latest_meeting = None\n",
    "latest_date = None\n",
    "\n",
    "# === Step 1: Find most recent past meeting block\n",
    "for block in meeting_blocks:\n",
    "    try:\n",
    "        month_text = block.find_element(By.CLASS_NAME, \"fomc-meeting__month\").text.strip()\n",
    "        day_text = block.find_element(By.CLASS_NAME, \"fomc-meeting__date\").text.strip()\n",
    "        first_day = int(re.findall(r\"\\d+\", day_text)[0])\n",
    "\n",
    "        year_match = re.search(r\"20\\d{2}\", block.get_attribute(\"innerHTML\"))\n",
    "        year = int(year_match.group()) if year_match else today.year\n",
    "        month_num = time.strptime(month_text, '%B').tm_mon\n",
    "        date_obj = datetime(year, month_num, first_day)\n",
    "\n",
    "        if date_obj <= today and (latest_date is None or date_obj > latest_date):\n",
    "            latest_meeting = block\n",
    "            latest_date = date_obj\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "print(f\"✅ Latest meeting selected: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "date_str = latest_date.strftime(\"%Y-%m-%d\")\n",
    "file_suffix = latest_date.strftime(\"%y%m\")\n",
    "combined_data = []\n",
    "\n",
    "# === Step 2: Parse the meeting block directly\n",
    "try:\n",
    "    soup = BeautifulSoup(latest_meeting.get_attribute(\"innerHTML\"), \"html.parser\")\n",
    "\n",
    "    # --- Statement ---\n",
    "    statement_link = soup.find(\"a\", href=re.compile(r\"monetary20\\d{6}a\\.htm\"))\n",
    "    if statement_link:\n",
    "        statement_url = urljoin(\"https://www.federalreserve.gov\", statement_link['href'])\n",
    "        response = requests.get(statement_url)\n",
    "        statement_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        statement_text = statement_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        combined_data.append({\n",
    "            \"date\": date_str,\n",
    "            \"type\": \"statement\",\n",
    "            \"url\": statement_url,\n",
    "            \"source_type\": \"HTML\",\n",
    "            \"content\": statement_text\n",
    "        })\n",
    "\n",
    "    # --- Minutes ---\n",
    "    minutes_link = soup.find(\"a\", href=re.compile(r\"fomcminutes20\\d{6}\\.htm\"))\n",
    "    if minutes_link:\n",
    "        minutes_url = urljoin(\"https://www.federalreserve.gov\", minutes_link['href'])\n",
    "        response = requests.get(minutes_url)\n",
    "        minutes_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        minutes_text = minutes_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        combined_data.append({\n",
    "            \"date\": date_str,\n",
    "            \"type\": \"minutes\",\n",
    "            \"url\": minutes_url,\n",
    "            \"source_type\": \"HTML\",\n",
    "            \"content\": minutes_text\n",
    "        })\n",
    "\n",
    "    # --- Press Conference Transcript PDF ---\n",
    "    pdf_link = soup.find(\"a\", href=re.compile(r\"/mediacenter/files/FOMCpresconf20\\d{6}\\.pdf\"))\n",
    "    if pdf_link:\n",
    "        pdf_url = urljoin(\"https://www.federalreserve.gov\", pdf_link['href'])\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            reader = PdfReader(BytesIO(response.content))\n",
    "            pdf_text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            combined_data.append({\n",
    "                \"date\": date_str,\n",
    "                \"type\": \"press_conference\",\n",
    "                \"url\": pdf_url,\n",
    "                \"source_type\": \"PDF\",\n",
    "                \"content\": pdf_text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ PDF extract failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to parse links from latest meeting: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# === Step 3: Save as ./fomc_YYMM.json\n",
    "output_path = f\"./fomc.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved all documents to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11835f6c",
   "metadata": {},
   "source": [
    "CNBC News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a59d9310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 13 articles within 7 days.\n",
      "- 2025-06-18: Stagflation on the Fed’s mind (https://www.cnbc.com/2025/06/19/cnbc-daily-open-stagflation-on-the-feds-mind.html)\n",
      "- 2025-06-18: Here’s how Wall Street is reacting to the Fed’s updated rate cut outlook (https://www.cnbc.com/2025/06/18/heres-how-wall-street-is-reacting-to-the-feds-updated-rate-cut-outlook.html)\n",
      "- 2025-06-18: Fed sees preferred inflation gauge topping 3%,  higher than previous forecast (https://www.cnbc.com/2025/06/18/federal-reserve-dot-plot-and-economic-projection-june-2025.html)\n",
      "- 2025-06-18: Here’s what changed in the new Fed statement (https://www.cnbc.com/2025/06/18/fed-meeting-heres-what-changed-in-the-new-statement.html)\n",
      "- 2025-06-18: Fed holds interest rates steady: Here’s what that means for your wallet (https://www.cnbc.com/2025/06/18/fed-holds-interest-rates-steady-what-that-means-for-your-money.html)\n",
      "- 2025-06-18: Fed holds key rate steady, still sees two more cuts this year (https://www.cnbc.com/2025/06/18/fed-rate-decision-june-2025-.html)\n",
      "- 2025-06-18: Trump says ‘stupid’ Powell ‘probably won’t cut’ rates when Fed meeting ends Wednesday (https://www.cnbc.com/2025/06/18/trump-says-stupid-powell-probably-wont-cut-rates-when-fed-meeting-ends-wednesday.html)\n",
      "- 2025-06-18: Is the U.S. consumer OK? Fed chief Powell will soon be the latest to weigh in (https://www.cnbc.com/2025/06/18/is-the-us-consumer-ok-fed-chief-powell-will-soon-be-the-latest-to-weigh-in.html)\n",
      "- 2025-06-18: The prospect of an Israel-Iran ceasefire dims as Trump weighs strikes (https://www.cnbc.com/2025/06/18/cnbc-daily-open-israel-iran-ceasefire-hope-dims-as-trump-weighs-strikes.html)\n",
      "- 2025-06-17: Fed chairs are tough on policy in final year, and Powell is no different: DataTrek’s Colas (https://www.cnbc.com/2025/06/17/fed-chairs-are-tough-on-policy-in-final-year-powell-too-datatreks-colas.html)\n",
      "- 2025-06-17: Investors see stagflation ahead but slow interest rate cuts, CNBC Fed survey finds (https://www.cnbc.com/2025/06/17/investor-see-stagflation-ahead-but-slow-interest-rate-cuts-cnbc-fed-survey-finds.html)\n",
      "- 2025-06-16: What the Fed’s upcoming decision on interest rates could mean for your money (https://www.cnbc.com/2025/06/16/fed-likely-to-hold-interest-rates-steady-what-that-means-for-you.html)\n",
      "- 2025-06-15: Here are the 4 big things we’re watching in the stock market in the week ahead (https://www.cnbc.com/2025/06/15/the-4-big-things-were-watching-in-the-stock-market-in-the-week-ahead.html)\n"
     ]
    }
   ],
   "source": [
    "# === Setup ===\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(\"https://www.cnbc.com/federal-reserve/\")\n",
    "time.sleep(5)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# === Time filter ===\n",
    "today = datetime.today()\n",
    "one_week_ago = today - timedelta(days=7)\n",
    "\n",
    "# === Article extraction ===\n",
    "articles = []\n",
    "\n",
    "# Find all article blocks\n",
    "for card in soup.find_all(\"div\", class_=\"Card-card\"):\n",
    "    title_tag = card.find(\"a\", class_=\"Card-title\")\n",
    "    date_tag = card.find(\"span\", class_=\"Card-time\")\n",
    "\n",
    "    if not title_tag or not date_tag:\n",
    "        continue\n",
    "\n",
    "    date_text = date_tag.get_text(strip=True)\n",
    "\n",
    "    try:\n",
    "        clean_date = date_text.replace('st', '').replace('nd', '').replace('rd', '').replace('th', '')\n",
    "        article_date = datetime.strptime(clean_date, \"%a, %b %d %Y\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if article_date < one_week_ago:\n",
    "        continue\n",
    "\n",
    "    articles.append({\n",
    "        \"title\": title_tag.text.strip(),\n",
    "        \"url\": title_tag[\"href\"],\n",
    "        \"date\": article_date.strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "\n",
    "print(f\"✅ Found {len(articles)} articles within 7 days.\")\n",
    "for a in articles:\n",
    "    print(f\"- {a['date']}: {a['title']} ({a['url']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        driver.get(article['url'])\n",
    "        time.sleep(2)\n",
    "        \n",
    "        page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        summary = page_soup.find_all('li')\n",
    "        paragraphs = page_soup.find_all('p')\n",
    "        \n",
    "        content_parts = []\n",
    "\n",
    "        content_parts.append(article['title'])\n",
    "        \n",
    "        if summary:\n",
    "            content_parts.append(\"Summary:\")\n",
    "            content_parts.extend(line.get_text(strip=True) for line in summary if line.get_text(strip=True))\n",
    "\n",
    "        if paragraphs:\n",
    "            content_parts.append(\"Body:\")\n",
    "            content_parts.extend(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "\n",
    "        content = '\\n'.join(content_parts)\n",
    "                \n",
    "        article_data = {\n",
    "            \"title\": article[\"title\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"date\": article[\"date\"],\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "        recent_articles.append(article_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {article['url']}: {e}\")\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b40a29be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all documents to ./cnbc.json\n"
     ]
    }
   ],
   "source": [
    "# === Step 3: Save as ./fomc_YYMM.json\n",
    "output_path = f\"./cnbc.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(recent_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved all documents to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcd97c",
   "metadata": {},
   "source": [
    "## Cleaning and validating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3dc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sentence splitter (basic) ===\n",
    "sentence_pattern = re.compile(r'(?<=[.!?]) +')\n",
    "\n",
    "# === Define keyword and splitting logic ===\n",
    "split_tokens = [\"but\", \"however\", \"even though\", \"although\", \"while\", \";\"]\n",
    "split_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, split_tokens)) + r\")\\b|;\")\n",
    "\n",
    "keywords = set(map(str.lower, [\n",
    "    # Panel A1\n",
    "    \"inflation expectation\", \"interest rate\", \"bank rate\", \"fund rate\", \"price\", \n",
    "    \"economic activity\", \"inflation\", \"employment\",\n",
    "    # Panel A2\n",
    "    \"anchor\", \"cut\", \"subdue\", \"decline\", \"decrease\", \"reduce\", \"low\", \"drop\", \"fall\",\n",
    "    \"fell\", \"decelerate\", \"slow\", \"pause\", \"pausing\", \"stable\", \"non-accelerating\", \n",
    "    \"downward\", \"tighten\",\n",
    "    # Panel B1\n",
    "    \"unemployment\", \"growth\", \"exchange rate\", \"productivity\", \"deficit\", \"demand\",\n",
    "    \"job market\", \"monetary policy\",\n",
    "    # Panel B2\n",
    "    \"ease\", \"easing\", \"rise\", \"rising\", \"increase\", \"expand\", \"improve\", \"strong\", \n",
    "    \"upward\", \"raise\", \"high\", \"rapid\"\n",
    "]))\n",
    "\n",
    "junk_phrases = [\n",
    "        \"cookie\", \"cookies\", \"terms of use\", \"privacy policy\", \"ads and content\", \n",
    "        \"by using this site\", \"subscribe\", \"sign up\", \"CNBC\", \"NBCUniversal\", \"copyright\",\n",
    "        \"click\", \"browser\", \"advertise with us\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e1e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Read two json files ===\n",
    "with open(\"./fomc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    fomc_data = json.load(f)\n",
    "\n",
    "with open(\"./cnbc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cnbc_data = json.load(f)\n",
    "    \n",
    "all_data = fomc_data + cnbc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56154a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 10 documents with filtered sentences to filtered_sentences_by_url.json\n"
     ]
    }
   ],
   "source": [
    "# === Result: sentences grouped by URL\n",
    "filtered_sentences_by_url = {}\n",
    "\n",
    "# === Process each item ===\n",
    "for item in all_data:\n",
    "    content = item.get(\"content\", \"\")\n",
    "    url = item.get(\"url\", \"unknown_source\")\n",
    "    source_type = item.get(\"type\", \"unknown_type\")\n",
    "\n",
    "    if not content.strip():\n",
    "        continue\n",
    "\n",
    "    # --- Split into sentences ---\n",
    "    sentences = sentence_pattern.split(content)\n",
    "\n",
    "    valid_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # --- Split further on split tokens ---\n",
    "        parts = split_pattern.split(sentence)\n",
    "        parts = [part.strip() for part in parts if part and not re.match(split_pattern, part)]\n",
    "\n",
    "        for part in parts:\n",
    "            if len(part.split()) < 3 or part.count('\\n') > 3 or len(re.findall(r'[.!?]', part)) < 1:\n",
    "                continue\n",
    "\n",
    "            part_lower = part.lower()\n",
    "\n",
    "            if any(junk_phrase in part_lower for junk_phrase in junk_phrases):\n",
    "                continue\n",
    "\n",
    "            if any(re.search(rf\"\\b{re.escape(keyword)}\\b\", part_lower) for keyword in keywords):\n",
    "                valid_sentences.append(part)\n",
    "\n",
    "    # === If this item had any valid sentences, save them by URL\n",
    "    if valid_sentences:\n",
    "        filtered_sentences_by_url[url] = {\n",
    "            \"type\": source_type,\n",
    "            \"sentences\": valid_sentences\n",
    "        }\n",
    "\n",
    "# === Save\n",
    "output_path = \"filtered_sentences_by_url.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_sentences_by_url, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(filtered_sentences_by_url)} documents with filtered sentences to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d730406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 66 valid sentences.\n",
      "- EDT\n",
      "Share\n",
      "Although swings in net exports have affected the data, recent indicators suggest that economic activity has continued to expand at a solid pace.\n",
      "- The unemployment rate remains low, and labor market conditions remain solid.\n",
      "- Inflation remains somewhat elevated.\n",
      "The Committee seeks to achieve maximum employment and inflation at the rate of 2 percent over the longer run.\n",
      "- The Committee is strongly committed to supporting maximum employment and returning inflation to its 2 percent objective.\n",
      "In assessing the appropriate stance of monetary policy, the Committee will continue to monitor the implications of incoming information for the economic outlook.\n",
      "- The Committee would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that could impede the attainment of the Committee's goals.\n",
      "- The Committee's assessments will take into account a wide range of information, including readings on labor market conditions, inflation pressures and inflation expectations, and financial and international developments.\n",
      "Voting for the monetary policy action were Jerome H.\n",
      "- Federal Reserve on Wednesday kept interest rates in a range between 4.25%-4.5% and kept two cuts in 2025 on the table.\n",
      "Inflation in the U.S., measured by the personal consumption expenditures price index, will rise beyond 3% in 2025, according to an updated Fed forecast.\n",
      "U.S.\n",
      "- Europe’s Stoxx 600 index fell.\n",
      "U.S President Donald Trump insisted he had not yet decided whether to order a U.S.\n",
      "- Federal Reserve Chair Jerome Powell’spost-meeting press conference, the topic of tariffs — specifically, their impact on prices — was a recurring one.\n",
      "“Everyone that I know is forecasting a meaningful increase in inflation in coming months from tariffs because someone has to pay for the tariffs,” Powell said.\n",
      "- “And some of it will fall on the end consumer.”\n",
      "Granted, recent economic data has been upbeat, suggesting the U.S.\n",
      "- economy has been able to — and could still — escape from tariffs mostly unscathed.\n",
      "In May, a better-than-expected139,000 jobs were addedand the unemployment rate was unchanged at 4.2%.Consumer sentiment in early Junewas much more optimistic than forecast, according to a University of Michigan survey.\n",
      "- And, most crucially,inflation in May— based on the consumer price index — ticked up just 0.1% for the month, lower than estimated.\n",
      "But that string of positive data might have to thank the slow process by which tariffs move through the economy.\n",
      "“It takes some time for tariffs to work their way through the chain of distribution to the end consumer.\n",
      "- economy weakening, Powell acknowledged growth will slow “eventually.” In other words, stagflation — the toxic mix of higher prices and slower growth — could be a possibility in the months ahead.\n",
      "The song “I Got Summer on My Mind” went viral in 2022.\n",
      "- Earlier Wednesday, President Donald Trump said the fed funds rate should be at least 2 percentage points lower, and again slammed Chair Jerome Powell,calling him “stupid.”\n",
      "Fed hikes inflation and lowers growth forecastsInflation in the U.S., measured by the personal consumption expenditures price index, will rise beyond 3% in 2025, according to an updated Fed forecast.\n",
      "- The Fed also sees economic growth slowing to 1.4% this year, down from an earlier estimate of 1.7%.\n",
      "- annual inflation coming in at anexpected 3.4% in May.\n",
      "Trump says he hasn’t decided on Iran strikesFor the second time in two days, Trump on Wednesdaymet his national security team in the White Houseamid the Israel-Iran conflict.\n",
      "- Ambassador to Israel Mike Huckabee said evacuation flights and cruise ship departures were being arranged for American citizens seeking to leave Israel.\n",
      "[PRO]‘Profound impact’ on oil market: JPMorganThe current jump in oil prices because of the simmering conflict in the Middle East might not lead to a long-term price shock, according to historical data analyzed byJPMorgan.\n",
      "- and European Union are running out of time to strike a deal on trade tariffs — and analysts say several key sticking points could make an agreement impossible.\n",
      "Negotiations have been slow since both the U.S.\n",
      "- and EU temporarily cut duties on each other until July 9.\n",
      "- Here’s how Wall Street is reacting to the Fed’s updated rate cut outlook\n",
      "Summary:\n",
      "Pre-Markets\n",
      "U.S.\n"
     ]
    }
   ],
   "source": [
    "# === Done ===\n",
    "filtered_sentences = []\n",
    "\n",
    "for url, data in filtered_sentences_by_url.items():\n",
    "    for sentence in data[\"sentences\"]:\n",
    "        filtered_sentences.append(sentence)\n",
    "    \n",
    "print(f\"✅ Found {len(filtered_sentences)} valid sentences.\")\n",
    "for s in filtered_sentences[:20]:  # preview first 20\n",
    "    print(\"-\", s)\n",
    "\n",
    "# === Save to json ===\n",
    "with open(\"processed_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_sentences, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5ffb7",
   "metadata": {},
   "source": [
    "## Loading roBERTa base model and Mistral model for classification and summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecc9bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Gemma on device: cpu\n",
      "✅ Loaded finetuned RoBERTa on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# === Load Mistral model (causal LM)\n",
    "save_dir = \"../models/saved_gemma_2_2b_it_model\"\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "# Move to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma_model = gemma_model.to(device)\n",
    "\n",
    "print(f\"✅ Loaded Gemma on device: {device}\")\n",
    "\n",
    "# === Load finetuned RoBERTa (sequence classification)\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"../models/finetuned_roberta_model_best_val_loss\")\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"../models/finetuned_roberta_model_best_val_loss\", num_labels=3)\n",
    "\n",
    "# Move to CUDA if available\n",
    "roberta_model = roberta_model.to(torch.float32).to(device) \n",
    "roberta_model.eval()\n",
    "\n",
    "print(f\"✅ Loaded finetuned RoBERTa on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f86554f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading second RoBERTa model, pre-overfit\n",
    "roberta_tokenizer_pre_overfit = AutoTokenizer.from_pretrained(\"../models/finetuned_roberta_model_pre_overfit_epoch_8\")\n",
    "roberta_model_pre_overfit = RobertaForSequenceClassification.from_pretrained(\"../models/finetuned_roberta_model_pre_overfit_epoch_8\", num_labels=3)\n",
    "\n",
    "# Move to CUDA if available\n",
    "roberta_model_pre_overfit = roberta_model_pre_overfit.to(torch.float32).to(device) \n",
    "roberta_model_pre_overfit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deee2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./processed_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    processed_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348cbf5a",
   "metadata": {},
   "source": [
    "Generating labels for each sentence using roBERTa base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a test sentence to check model's accuracy\n",
    "sentence = \"The Fed is going to lower interest rates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe47b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = roberta_tokenizer_pre_overfit(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = roberta_model_pre_overfit(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax(dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc165cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "370ceb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:01<00:00, 49.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 66 sentence labels to roberta_sentence_labels.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Process sentences ===\n",
    "results = []\n",
    "\n",
    "for sentence in tqdm(processed_sentences):\n",
    "    inputs = roberta_tokenizer_pre_overfit(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model_pre_overfit(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = logits.argmax(dim=-1).item()\n",
    "\n",
    "    # OPTIONAL: map label to text (depends on your training labels!)\n",
    "    label_map = {\n",
    "        0: \"hawkish\",\n",
    "        1: \"dovish\",\n",
    "        2: \"neutral\"\n",
    "    }\n",
    "    label_text = label_map.get(predicted_class_id, str(predicted_class_id))\n",
    "\n",
    "    results.append({\n",
    "        \"sentence\": sentence,\n",
    "        \"label_id\": predicted_class_id,\n",
    "        \"label\": label_text\n",
    "    })\n",
    "\n",
    "# === Save to JSON ===\n",
    "output_path = \"roberta_sentence_labels.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(results)} sentence labels to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d944984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./roberta_sentence_labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    roberta_sentence_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1416755",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hawkish, num_dovish = 0, 0 \n",
    "num_total_sentences = len(processed_sentences)\n",
    "\n",
    "for sentence in roberta_sentence_labels:\n",
    "    if sentence['label_id'] == 0:\n",
    "        num_hawkish += 1\n",
    "    elif sentence['label_id'] == 1:\n",
    "        num_dovish += 1\n",
    "    \n",
    "index = (num_hawkish - num_dovish) / num_total_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d4307ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hawkish sentences: 14, number of dovish sentences: 21, number of neutral sentences: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of hawkish sentences: {num_hawkish}, number of dovish sentences: {num_dovish}, number of neutral sentences: {num_total_sentences - num_hawkish - num_dovish}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f6911c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10606060606060606"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An index < 0 implies that there are more dovish sentences than hawkish sentences\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a3fee",
   "metadata": {},
   "source": [
    "Generating summary for CNBC articles and FOMC sentences using Gemma 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cffbebe",
   "metadata": {},
   "source": [
    "Formatting sentences in roberta_sentence_labels.json into LLM-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d04278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lines = []\n",
    "for i, entry in enumerate(roberta_sentence_labels):\n",
    "    sentence_lines.append(f\"[{i+1}] \\\"{entry['sentence']}\\\" → Label: {entry['label'].capitalize()}\")\n",
    "\n",
    "sentences_text_block = \"\\n\".join(sentence_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4670a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1] \"EDT\\nShare\\nAlthough swings in net exports have affected the data, recent indicators suggest that economic activity has continued to expand at a solid pace.\" → Label: Neutral\\n[2] \"The unemployment rate remains low, and labor market conditions remain solid.\" → Label: Neutral\\n[3] \"Inflation remains somewhat elevated.\\nThe Committee seeks to achieve maximum employment and inflation at the rate of 2 percent over the longer run.\" → Label: Neutral\\n[4] \"The Committee is strongly committed to supporting maximum employment and returning inflation to its 2 percent objective.\\nIn assessing the appropriate stance of monetary policy, the Committee will continue to monitor the implications of incoming information for the economic outlook.\" → Label: Neutral\\n[5] \"The Committee would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that could impede the attainment of the Committee\\'s goals.\" → Label: Neutral\\n[6] \"The Committee\\'s assessments will take into account a wide range of information, including readings on labor market conditions, inflation pressures and inflation expectations, and financial and international developments.\\nVoting for the monetary policy action were Jerome H.\" → Label: Neutral\\n[7] \"Federal Reserve on Wednesday kept interest rates in a range between 4.25%-4.5% and kept two cuts in 2025 on the table.\\nInflation in the U.S., measured by the personal consumption expenditures price index, will rise beyond 3% in 2025, according to an updated Fed forecast.\\nU.S.\" → Label: Hawkish\\n[8] \"Europe’s Stoxx 600 index fell.\\nU.S President Donald Trump insisted he had not yet decided whether to order a U.S.\" → Label: Neutral\\n[9] \"Federal Reserve Chair Jerome Powell’spost-meeting press conference, the topic of tariffs — specifically, their impact on prices — was a recurring one.\\n“Everyone that I know is forecasting a meaningful increase in inflation in coming months from tariffs because someone has to pay for the tariffs,” Powell said.\" → Label: Hawkish\\n[10] \"“And some of it will fall on the end consumer.”\\nGranted, recent economic data has been upbeat, suggesting the U.S.\" → Label: Dovish\\n[11] \"economy has been able to — and could still — escape from tariffs mostly unscathed.\\nIn May, a better-than-expected139,000 jobs were addedand the unemployment rate was unchanged at 4.2%.Consumer sentiment in early Junewas much more optimistic than forecast, according to a University of Michigan survey.\" → Label: Neutral\\n[12] \"And, most crucially,inflation in May— based on the consumer price index — ticked up just 0.1% for the month, lower than estimated.\\nBut that string of positive data might have to thank the slow process by which tariffs move through the economy.\\n“It takes some time for tariffs to work their way through the chain of distribution to the end consumer.\" → Label: Neutral\\n[13] \"economy weakening, Powell acknowledged growth will slow “eventually.” In other words, stagflation — the toxic mix of higher prices and slower growth — could be a possibility in the months ahead.\\nThe song “I Got Summer on My Mind” went viral in 2022.\" → Label: Dovish\\n[14] \"Earlier Wednesday, President Donald Trump said the fed funds rate should be at least 2 percentage points lower, and again slammed Chair Jerome Powell,calling him “stupid.”\\nFed hikes inflation and lowers growth forecastsInflation in the U.S., measured by the personal consumption expenditures price index, will rise beyond 3% in 2025, according to an updated Fed forecast.\" → Label: Hawkish\\n[15] \"The Fed also sees economic growth slowing to 1.4% this year, down from an earlier estimate of 1.7%.\" → Label: Dovish\\n[16] \"annual inflation coming in at anexpected 3.4% in May.\\nTrump says he hasn’t decided on Iran strikesFor the second time in two days, Trump on Wednesdaymet his national security team in the White Houseamid the Israel-Iran conflict.\" → Label: Neutral\\n[17] \"Ambassador to Israel Mike Huckabee said evacuation flights and cruise ship departures were being arranged for American citizens seeking to leave Israel.\\n[PRO]‘Profound impact’ on oil market: JPMorganThe current jump in oil prices because of the simmering conflict in the Middle East might not lead to a long-term price shock, according to historical data analyzed byJPMorgan.\" → Label: Neutral\\n[18] \"and European Union are running out of time to strike a deal on trade tariffs — and analysts say several key sticking points could make an agreement impossible.\\nNegotiations have been slow since both the U.S.\" → Label: Dovish\\n[19] \"and EU temporarily cut duties on each other until July 9.\" → Label: Neutral\\n[20] \"Here’s how Wall Street is reacting to the Fed’s updated rate cut outlook\\nSummary:\\nPre-Markets\\nU.S.\" → Label: Neutral\\n[21] \"Fed sees preferred inflation gauge topping 3%,  higher than previous forecast\\nSummary:\\nPre-Markets\\nU.S.\" → Label: Hawkish\\n[22] \"The latter is a gauge Fed officials believe to be a better measure of longer-term trends.\\nCentral bank officials also see further slowing in economic growth, projecting the gross domestic project expanding just 1.4% this year.\" → Label: Dovish\\n[23] \"In March, they expected a 1.7% pace in GDP growth.\\nFed ChairJerome Powellsaid in the post-meeting news conference that the recent uptick in inflation expectations could be tied to tariffs.\\n“Everyone that I know is forecasting a meaningful increase in inflation in coming months from tariffs because someone has to pay for the tariffs,” Powell said.\" → Label: Hawkish\\n[24] \"And some of it will fall on the end consumer,” he said.\\nFed officials have been reluctant to lower rates, worrying that Trump’s tariffs could cause inflation to reaccelerate in the coming months.\\xa0The conflict between Israel and Iran adds another wild card to the policy mix, as high oil prices could prevent the Fed from easing policy.\\nStill, the so-called dot plot — which indicates individual members’ expectations for rates — showed officials see their benchmark lending rate falling to 3.9% by the end of 2025.\" → Label: Dovish\\n[25] \"“With the uncertainty around tariffs and how that could impact inflation readings in the month ahead, there’s an ongoing sense of another shoe about to drop,” McBride said.\\nMore from Personal Finance:Welcome to the zoo.\" → Label: Neutral\\n[26] \"Even though the central banklowered its benchmark ratethree times in 2024, those consumer rates are still elevated, and are mostly staying high, for now.\\n“Borrowing rates are high, with mortgage rates near 7%, many home equity lines of credit in double-digit interest rate territory, and the average credit card rate still above 20%,” McBride said.\" → Label: Hawkish\\n[27] \"“But savers continue to be rewarded with inflation-beating returns on the top-yielding savings accounts, money market accounts, and certificates of deposit.\" → Label: Neutral\\n[28] \"Retirees, in particular, are earning good income on their hard-earned savings.”\\nManycredit cardshave a variable rate, so there’s a direct connection to the Fed’s benchmark.\\nWith a rate cut likely postponeduntil at least September, the\\xa0average credit card annual percentage rate is currently just over 20%, according to Bankrate — not far from last year’sall-time high.\" → Label: Neutral\\n[29] \"the Fed is one of the most significant.\\nWith the Fed’s benchmark holding steady, the average rate on a five-year new car loan was 7.3% in May, near a record high,\" → Label: Neutral\\n[30] \"the average auto loan rate forused carswas 11%, according to Edmunds.\\nButcar\\xa0pricesare also rising — in part due to pressure from Trump’stariffs on imported vehicles— leaving car buyers with bigger monthly payments and a growing affordability problem.\" → Label: Neutral\\n[31] \"Starting July 1, the interest rates will be 6.39%.\\nAlthough borrowers with existing federalstudent debtbalances won’t see their rates change, many are now facing otherheadwindsand fewerfederal loan forgivenessoptions.\\nWhile the central bank has no\\xa0direct influence\\xa0on deposit rates, the yields tend to be correlated to changes in the target federal funds rate.\\n“Yields for\\xa0CDs\\xa0and\\xa0high-yield savings accounts\\xa0aren’t at the sky-high levels they were a year ago,\" → Label: Dovish\\n[32] \"they’re still really strong,” said LendingTree’s Schulz.\" → Label: Neutral\\n[33] \"However, the committee approved the policy statement unanimously.\\nEconomic projections from meeting participants pointed to further stagflationary pressures, with participants seeing the gross domestic product advancing at a 1.4% pace in 2025 and inflation hitting 3%.\\nThe revised forecasts from the last update in March represented a decrease of 0.3 percentage point for GDP and an increase of the same amount for the personal consumption expenditures price index.\" → Label: Dovish\\n[34] \"The unemployment outlook saw a small revision, up to 4.5%, or 0.1 percentage point higher than March and 0.3 percentage point above the current level.\\nThe FOMC statement changed little from the May meeting.\" → Label: Hawkish\\n[35] \"Broadly speaking, the economy grew at a “solid pace,” with “low” unemployment and “somewhat elevated” inflation, the committee said.\\nMoreover, the committee indicated less concern about the gyrations of the economy and the clouds over White House trade policy.\\n“Uncertainty about the economic outlook has diminished\" → Label: Neutral\\n[36] \", has not softened.\\nEarlier Wednesday, the president again slammed Powell and his colleagues for not easing.\" → Label: Dovish\\n[37] \"Trump said the fed funds rate should be at least 2 percentage points lower andderided Powell as “stupid”for not pushing the committee to cut.\\nFed officials have been reluctant to move, fearful that tariffs Trump implemented this year could causeinflation in the coming months.\" → Label: Dovish\\n[38] \"Price gauges so far have not indicated that the duties are having much of an impact.\" → Label: Neutral\\n[39] \"A delay in feed-through of the tariffs along with softening consumer demand and a buildup of inventories ahead of the April 2 “liberation day” announcement havehelped deflect their impact.\\n“Everyone that I know is forecasting a meaningful increase in inflation in coming months from tariffs because someone has to pay for the tariffs,” Powell said.\\nTheconflict between Israel and Iranadds another wild card to the policy mix, withprospects of higher energy pricesa potential additional factor in keeping the Fed from cutting.\" → Label: Dovish\\n[40] \"The statement did not mention influence from the Middle East fighting.\\nA gradually softening economy could provide incentive to cut later this year.\\nRecent labor market data shows layoffs creeping higher, long-term unemployment also rising and consumers spending less.Retail sales tumbled nearly 1% in Mayand recent data has reflected a cooling housing market, with starts hitting their lowest level in five years.\\n“Effectively they are sitting on their hands, waiting to see if tariffs increase inflation or the jobs market starts to falter, and whichever part of their dual mandate is impacted first will likely guide whichever direction they take,\" → Label: Dovish\\n[41] \"However, he said the market was surprised by the comment that uncertainty had “diminished.”\\nFor Trump, though, the importance of lower rates stems from the high cost the government is paying to finance its $36 trillion debt.\\nInterest on the debt is on track to total $1.2 trillion this year and exceeds all other budget items except Social Security and Medicare.\" → Label: Neutral\\n[42] \"The Fed last cut in December, and Treasury yields have held higher throughout the year, putting additional pressure ona budget deficitlikely to approach $2 trillion, or more than 6% of GDP.\\nCorrection: The meeting participants expect gross domestic product to advance at a 1.4% pace in 2025.\" → Label: Hawkish\\n[43] \"Trump says ‘stupid’ Powell ‘probably won’t cut’ rates when Fed meeting ends Wednesday\\nSummary:\\nPre-Markets\\nU.S.\" → Label: Dovish\\n[44] \"doubting the central bank would cut.\\nIn his latest in a series of attacks on Powell that go back years, Trump said the Fed’s key borrowing rate should be at least 2 percentage points lower.\\n“So we have a stupid person.\" → Label: Dovish\\n[45] \"Frankly, you probably won’t cut today,” Trump said in impromptu remarks just outside the White House.\" → Label: Neutral\\n[46] \"he’s costing the country a fortune.”\\nThe remarks came just about four hours before the rate-setting Federal Open Market Committee was to release its statement on interest rates, along with an update of where it sees policy and several key economic measures heading over the next several years.\\nMarket pricing indicates no probability of a cut at this meeting, with the next move expected in September.\" → Label: Neutral\\n[47] \"“hundreds of billions” of dollars in financing costs that could be saved if the Fed would ease.\\n“If he’s worried about inflation, that’s OK.\" → Label: Hawkish\\n[48] \"Singapore time, Japan’sNikkei 225rose 0.77% even as the government reported a drop in exports in May,\" → Label: Neutral\\n[49] \", lost 1.23%.\\nExports from Japan fall in MayJapan exports in Maydeclined 1.7% year over year, according todata from Japan’s trade ministryreleased Wednesday.\" → Label: Neutral\\n[50] \"While that drop fares better than the 3.8% decline expected from a Reuters poll of economists, it’s still the steepest fall since September 2024 and reverses the 2% growth in April.\" → Label: Dovish\\n[51] \"dropped 11.1% from a year earlier,much worse than than the 1.8% fall in April.\\nMeta trying to poach OpenAI staff: AltmanOn apodcastreleased Tuesday, OpenAI CEO Sam Altman said Meta hadsought to hire “a lot of people”from the artificial intelligence company, and had offered signing bonuses as high as $100 million —\" → Label: Hawkish\\n[52] \"What to expect\\nWhile any immediate movement on interest rates seems improbable, the Federal Reserve’s policy meeting, which concludes Wednesday, will feature important signals that still could move markets.\\nAmong the biggest things to watch will be whether Federal Open Market Committee members stick with their previous forecast of two rate cuts this year, how they see inflation trending, and any reaction from ChairJerome Powellto what has become a concerted White House campaign for easier monetary policy.\\nAs things stand heading into the meeting, markets are pricing in the next cut to come in September, which would be the one-year anniversary of a surprisingly aggressivehalf-percentage-point reductionthe FOMC instituted amid concerns over the labor market.\" → Label: Dovish\\n[53] \"slow interest rate cuts, CNBC Fed survey finds\\nSummary:\\nPre-Markets\\nU.S.\" → Label: Neutral\\n[54] \"this will require some help in mitigating some of these sources of volatility.”\\nThe help, he said, comes from trade deals and tax policy, with 29% saying the current tax bill in Congress makes them more optimistic about growth and 29% say it makes them more pessimistic\" → Label: Neutral\\n[55] \"43% say it has no effect on their growth outlook.\\nYet 82% believe it will “somewhat” or “significantly increase” the federal budget deficit.\\nA 54% majority expect the U.S.\" → Label: Hawkish\\n[56] \"On average, those looking for a trade deal believe it will be signed within about five months.\\n“While it appears that the worst-case scenario when it comes to tariffs will not happen, the best-case scenario still implies significantly higher tariffs and therefore higher inflation for an extended period,″ said Joel Naroff, president, Naroff Economics.\\nThe Federal Reserve isexpected to stand pat at the June meetingand not cut rates until September.\" → Label: Dovish\\n[57] \"only one 25 basis point cut next year.\\n“There are multiple cross currents from geopolitical events that the Fed will have to weigh,” said Constance Hunter, chief economist at the Economist Intelligence Unit.\" → Label: Hawkish\\n[58] \"we expect slower growth will ultimately be what causes the Fed to move closer to a neutral stance.”\\nAsked how the Fed would react to astagflationary outcomeof higher prices and weaker growth, 54% believe the central bank will cut rates\" → Label: Dovish\\n[59] \"That’s a bit more hawkish than May when 65% saw the Fed cutting.\\nBy a slim margin, respondents don’t believe the Fed will face that dilemma: 43% believe tariffs will result in just “one-time price increases”\" → Label: Dovish\\n[60] \"32% say they create a broader inflationary problem.\\nHowever, 61% say tariff inflation in future months will show up “more significantly” than it has,\" → Label: Hawkish\\n[61] \"36% see it as appearing “only modestly.”\\n“I expect (economic) moderation going forward as tariffs get incorporated into consumer prices and slower job growth due in part to a slower pace of immigration providing less fuel for aggregate disposable income,” said Jack Kleinhenz, chief economist at the National Retail Federation.\\nAmid all those concerns, Mark Vitner, chief economist at Piedmont Crescent Capital & CAVU Securities, notes that the economy has remained “Resilient, Not ‘Tariffied.’”\\n“There’s no shortage of reasons to worry — elevated interest rates, huge budget deficits, geopolitical flare-ups, tariff threats —\" → Label: Dovish\\n[62] \"economy keeps proving its doubters wrong.”\\nHe added, “Consumers continue to spend, and businesses are doubling down on future-facing infrastructure: AI, life sciences, DefenseTech, and other emerging technologies.”\\nIndeed, the outlook for stocks rose, with the S&P 500 now expected to hit 6,133 by year-end, for a modest 1.7% rise from current levels,\" → Label: Neutral\\n[63] \"a more robust 10% increase to 6,625 by the end of 2026.\\n“Equity markets are near all-time highs in anticipation of certainty replacing uncertainty on tariffs, the 2017 tax cuts getting extended, deregulation, AI productivity, and Fed easing,” said Hank Smith, head of investment strategy, Haverford Trust Co.\" → Label: Hawkish\\n[64] \"The\\xa0average annual percentage rate is currently just over 20%, according to Bankrate, not far from last year’sall-time high.\\n“This is a sign of banks trying to protect themselves from the risk that is out there in these uncertain times,” Schulz said.\" → Label: Neutral\\n[65] \"Tack on thenationwide problemof limited inventory andhousing affordabilityremains a key issue, regardless of the Fed’s next move.\\n“I don’t see any major changes coming in the immediate future, meaning that those shopping for a home this summer should expect rates to remain relatively high,” Schulz said.\\nAuto loan ratesare fixed, and not directly tied to the Fed.\" → Label: Dovish\\n[66] \"But payments are getting bigger becausecar pricesare rising, in part due to impacts from Trump’strade policy.\\nCurrently, the average rate on a five-year new car loan is 7.24%, according to Bankrate.\\nThe growth in median car payments is outpacing both new and used car prices, according to separate data from Bank of America.\" → Label: Neutral'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_text_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "# Gemma-2's context window is too small, mistral-7b is too large to run on GPU, TinyLlama is too slow\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tiny_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tiny_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# Move to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tiny_model = tiny_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bdbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Build input text\n",
    "user_message = f\"\"\"\n",
    "Given the following sentences and their sentiment classifications, summarise the overall monetary policy stance of the Fed.\n",
    "\n",
    "The index is calculated as: (number of hawkish sentences - number of dovish sentences) / (total number of sentences).\n",
    "A positive index (> 0) indicates an overall hawkish stance.\n",
    "A negative index (< 0) indicates an overall dovish stance.\n",
    "An index close to 0 indicates a neutral stance.\n",
    "\n",
    "Please consider both the index and the provided sentences in your reasoning.\n",
    "\n",
    "Index value: {index:.2f}\n",
    "\n",
    "Sentences:\n",
    "{sentences_text_block}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Call OpenAI API (GPT-3.5 Turbo)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a monetary policy expert.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    max_tokens=500,  # Limit for summary length — adjust if needed\n",
    "    temperature=0  # Adjust for creativity (lower = more factual)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92db47a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall monetary policy stance of the Fed, based on the provided sentences and sentiment classifications, is slightly dovish. The index value of -0.11 indicates a leaning towards a dovish stance. While there are some hawkish sentiments expressed in the sentences related to inflation concerns and the potential impact of tariffs on prices, there are also dovish sentiments regarding the potential need for rate cuts due to economic slowdown, uncertainties related to tariffs, and concerns about slower growth. The neutral sentiments in many sentences also contribute to the overall slightly dovish stance.\n"
     ]
    }
   ],
   "source": [
    "summary = response.choices[0].message.content\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f320b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_context_length = 10000\n",
    "\n",
    "def summarise_all(sentences_text_block, index):\n",
    "    formatted_input = (\n",
    "        \"<bos><start_of_turn>user\\n\"\n",
    "        \"You are a monetary policy expert.\\n\"\n",
    "        \"Given the following sentences and their sentiment classifications, summarise the overall monetary policy stance of the Fed.\\n\\n\"\n",
    "        f\"The index is calculated as: (number of hawkish sentences - number of dovish sentences) / (total number of sentences).\\n\"\n",
    "        \"A positive index (> 0) indicates an overall hawkish stance.\\n\"\n",
    "        \"A negative index (< 0) indicates an overall dovish stance.\\n\"\n",
    "        \"An index close to 0 indicates a neutral stance.\\n\"\n",
    "        \"Please consider both the index and the provided sentences in your reasoning.\\n\\n\"\n",
    "        f\"Index value: {index:.2f}\\n\\n\"\n",
    "        f\"Sentences:\\n{sentences_text_block}\\n\\n\"\n",
    "        \"Summary:<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenise\n",
    "    inputs = tiny_tokenizer(\n",
    "        formatted_input, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=max_context_length\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = tiny_model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "    response = tiny_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract generated summary\n",
    "    summary = response.split(\"Summary:\")[-1].strip()\n",
    "\n",
    "    print(\"=== Summary ===\")\n",
    "    print(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43ce40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\teomi\\Projects\\FEDDIE\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummarise_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_text_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m, in \u001b[0;36msummarise_all\u001b[1;34m(sentences_text_block, index)\u001b[0m\n\u001b[0;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tiny_tokenizer(\n\u001b[0;32m     20\u001b[0m     formatted_input, \n\u001b[0;32m     21\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     22\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     23\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_context_length\n\u001b[0;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tiny_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m tiny_tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Extract generated summary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teomi\\Projects\\FEDDIE\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\teomi\\Projects\\FEDDIE\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1755\u001b[0m     )\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   1759\u001b[0m         input_ids,\n\u001b[0;32m   1760\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1761\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   1762\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1763\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   1764\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1765\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1766\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1767\u001b[0m     )\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\teomi\\Projects\\FEDDIE\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2392\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2389\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2390\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m-> 2392\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2396\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teomi\\Projects\\FEDDIE\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1922\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[1;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[0;32m   1920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[0;32m   1923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summarise_all(sentences_text_block, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a49524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
