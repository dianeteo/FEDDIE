{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a47653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using Selenium for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Using transformers to load models\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfc9b7",
   "metadata": {},
   "source": [
    "## Loading most recent statements/transcripts/minutes/articles from websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4003a1",
   "metadata": {},
   "source": [
    "FOMC meeting minutes/press conference transcripts/statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8e6119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Latest meeting selected: 2025-06-17\n",
      "✅ Saved all documents to ./fomc.json\n"
     ]
    }
   ],
   "source": [
    "# === Setup headless Selenium\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(\"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\")\n",
    "time.sleep(2)\n",
    "\n",
    "# === Grab all meeting blocks\n",
    "meeting_blocks = driver.find_elements(By.CSS_SELECTOR, \".fomc-meeting\")\n",
    "today = datetime.today()\n",
    "\n",
    "latest_meeting = None\n",
    "latest_date = None\n",
    "\n",
    "# === Step 1: Find most recent past meeting block\n",
    "for block in meeting_blocks:\n",
    "    try:\n",
    "        month_text = block.find_element(By.CLASS_NAME, \"fomc-meeting__month\").text.strip()\n",
    "        day_text = block.find_element(By.CLASS_NAME, \"fomc-meeting__date\").text.strip()\n",
    "        first_day = int(re.findall(r\"\\d+\", day_text)[0])\n",
    "\n",
    "        year_match = re.search(r\"20\\d{2}\", block.get_attribute(\"innerHTML\"))\n",
    "        year = int(year_match.group()) if year_match else today.year\n",
    "        month_num = time.strptime(month_text, '%B').tm_mon\n",
    "        date_obj = datetime(year, month_num, first_day)\n",
    "\n",
    "        if date_obj <= today and (latest_date is None or date_obj > latest_date):\n",
    "            latest_meeting = block\n",
    "            latest_date = date_obj\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "print(f\"✅ Latest meeting selected: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "date_str = latest_date.strftime(\"%Y-%m-%d\")\n",
    "file_suffix = latest_date.strftime(\"%y%m\")\n",
    "combined_data = []\n",
    "\n",
    "# === Step 2: Parse the meeting block directly\n",
    "try:\n",
    "    soup = BeautifulSoup(latest_meeting.get_attribute(\"innerHTML\"), \"html.parser\")\n",
    "\n",
    "    # --- Statement ---\n",
    "    statement_link = soup.find(\"a\", href=re.compile(r\"monetary20\\d{6}a\\.htm\"))\n",
    "    if statement_link:\n",
    "        statement_url = urljoin(\"https://www.federalreserve.gov\", statement_link['href'])\n",
    "        response = requests.get(statement_url)\n",
    "        statement_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        statement_text = statement_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        combined_data.append({\n",
    "            \"date\": date_str,\n",
    "            \"type\": \"statement\",\n",
    "            \"url\": statement_url,\n",
    "            \"source_type\": \"HTML\",\n",
    "            \"content\": statement_text\n",
    "        })\n",
    "\n",
    "    # --- Minutes ---\n",
    "    minutes_link = soup.find(\"a\", href=re.compile(r\"fomcminutes20\\d{6}\\.htm\"))\n",
    "    if minutes_link:\n",
    "        minutes_url = urljoin(\"https://www.federalreserve.gov\", minutes_link['href'])\n",
    "        response = requests.get(minutes_url)\n",
    "        minutes_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        minutes_text = minutes_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        combined_data.append({\n",
    "            \"date\": date_str,\n",
    "            \"type\": \"minutes\",\n",
    "            \"url\": minutes_url,\n",
    "            \"source_type\": \"HTML\",\n",
    "            \"content\": minutes_text\n",
    "        })\n",
    "\n",
    "    # --- Press Conference Transcript PDF ---\n",
    "    pdf_link = soup.find(\"a\", href=re.compile(r\"/mediacenter/files/FOMCpresconf20\\d{6}\\.pdf\"))\n",
    "    if pdf_link:\n",
    "        pdf_url = urljoin(\"https://www.federalreserve.gov\", pdf_link['href'])\n",
    "        try:\n",
    "            response = requests.get(pdf_url)\n",
    "            reader = PdfReader(BytesIO(response.content))\n",
    "            pdf_text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            combined_data.append({\n",
    "                \"date\": date_str,\n",
    "                \"type\": \"press_conference\",\n",
    "                \"url\": pdf_url,\n",
    "                \"source_type\": \"PDF\",\n",
    "                \"content\": pdf_text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ PDF extract failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to parse links from latest meeting: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# === Step 3: Save as ./fomc_YYMM.json\n",
    "output_path = f\"./fomc.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved all documents to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11835f6c",
   "metadata": {},
   "source": [
    "CNBC News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a59d9310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 13 articles within 7 days.\n",
      "- 2025-06-18: Stagflation on the Fed’s mind (https://www.cnbc.com/2025/06/19/cnbc-daily-open-stagflation-on-the-feds-mind.html)\n",
      "- 2025-06-18: Here’s how Wall Street is reacting to the Fed’s updated rate cut outlook (https://www.cnbc.com/2025/06/18/heres-how-wall-street-is-reacting-to-the-feds-updated-rate-cut-outlook.html)\n",
      "- 2025-06-18: Fed sees preferred inflation gauge topping 3%,  higher than previous forecast (https://www.cnbc.com/2025/06/18/federal-reserve-dot-plot-and-economic-projection-june-2025.html)\n",
      "- 2025-06-18: Here’s what changed in the new Fed statement (https://www.cnbc.com/2025/06/18/fed-meeting-heres-what-changed-in-the-new-statement.html)\n",
      "- 2025-06-18: Fed holds interest rates steady: Here’s what that means for your wallet (https://www.cnbc.com/2025/06/18/fed-holds-interest-rates-steady-what-that-means-for-your-money.html)\n",
      "- 2025-06-18: Fed holds key rate steady, still sees two more cuts this year (https://www.cnbc.com/2025/06/18/fed-rate-decision-june-2025-.html)\n",
      "- 2025-06-18: Trump says ‘stupid’ Powell ‘probably won’t cut’ rates when Fed meeting ends Wednesday (https://www.cnbc.com/2025/06/18/trump-says-stupid-powell-probably-wont-cut-rates-when-fed-meeting-ends-wednesday.html)\n",
      "- 2025-06-18: Is the U.S. consumer OK? Fed chief Powell will soon be the latest to weigh in (https://www.cnbc.com/2025/06/18/is-the-us-consumer-ok-fed-chief-powell-will-soon-be-the-latest-to-weigh-in.html)\n",
      "- 2025-06-18: The prospect of an Israel-Iran ceasefire dims as Trump weighs strikes (https://www.cnbc.com/2025/06/18/cnbc-daily-open-israel-iran-ceasefire-hope-dims-as-trump-weighs-strikes.html)\n",
      "- 2025-06-17: Fed chairs are tough on policy in final year, and Powell is no different: DataTrek’s Colas (https://www.cnbc.com/2025/06/17/fed-chairs-are-tough-on-policy-in-final-year-powell-too-datatreks-colas.html)\n",
      "- 2025-06-17: Investors see stagflation ahead but slow interest rate cuts, CNBC Fed survey finds (https://www.cnbc.com/2025/06/17/investor-see-stagflation-ahead-but-slow-interest-rate-cuts-cnbc-fed-survey-finds.html)\n",
      "- 2025-06-16: What the Fed’s upcoming decision on interest rates could mean for your money (https://www.cnbc.com/2025/06/16/fed-likely-to-hold-interest-rates-steady-what-that-means-for-you.html)\n",
      "- 2025-06-15: Here are the 4 big things we’re watching in the stock market in the week ahead (https://www.cnbc.com/2025/06/15/the-4-big-things-were-watching-in-the-stock-market-in-the-week-ahead.html)\n"
     ]
    }
   ],
   "source": [
    "# === Setup ===\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(\"https://www.cnbc.com/federal-reserve/\")\n",
    "time.sleep(5)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# === Time filter ===\n",
    "today = datetime.today()\n",
    "one_week_ago = today - timedelta(days=7)\n",
    "\n",
    "# === Article extraction ===\n",
    "articles = []\n",
    "\n",
    "# Find all article blocks\n",
    "for card in soup.find_all(\"div\", class_=\"Card-card\"):\n",
    "    title_tag = card.find(\"a\", class_=\"Card-title\")\n",
    "    date_tag = card.find(\"span\", class_=\"Card-time\")\n",
    "\n",
    "    if not title_tag or not date_tag:\n",
    "        continue\n",
    "\n",
    "    date_text = date_tag.get_text(strip=True)\n",
    "\n",
    "    try:\n",
    "        clean_date = date_text.replace('st', '').replace('nd', '').replace('rd', '').replace('th', '')\n",
    "        article_date = datetime.strptime(clean_date, \"%a, %b %d %Y\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if article_date < one_week_ago:\n",
    "        continue\n",
    "\n",
    "    articles.append({\n",
    "        \"title\": title_tag.text.strip(),\n",
    "        \"url\": title_tag[\"href\"],\n",
    "        \"date\": article_date.strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "\n",
    "print(f\"✅ Found {len(articles)} articles within 7 days.\")\n",
    "for a in articles:\n",
    "    print(f\"- {a['date']}: {a['title']} ({a['url']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        driver.get(article['url'])\n",
    "        time.sleep(2)\n",
    "        \n",
    "        page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        summary = page_soup.find_all('li')\n",
    "        paragraphs = page_soup.find_all('p')\n",
    "        \n",
    "        content_parts = []\n",
    "\n",
    "        content_parts.append(article['title'])\n",
    "        \n",
    "        if summary:\n",
    "            content_parts.append(\"Summary:\")\n",
    "            content_parts.extend(line.get_text(strip=True) for line in summary if line.get_text(strip=True))\n",
    "\n",
    "        if paragraphs:\n",
    "            content_parts.append(\"Body:\")\n",
    "            content_parts.extend(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "\n",
    "        content = '\\n'.join(content_parts)\n",
    "                \n",
    "        article_data = {\n",
    "            \"title\": article[\"title\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"date\": article[\"date\"],\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "        recent_articles.append(article_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {article['url']}: {e}\")\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b40a29be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all documents to ./cnbc.json\n"
     ]
    }
   ],
   "source": [
    "# === Step 3: Save as ./fomc_YYMM.json\n",
    "output_path = f\"./cnbc.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(recent_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved all documents to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcd97c",
   "metadata": {},
   "source": [
    "## Cleaning and validating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3dc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sentence splitter (basic) ===\n",
    "sentence_pattern = re.compile(r'(?<=[.!?]) +')\n",
    "\n",
    "# === Define keyword and splitting logic ===\n",
    "split_tokens = [\"but\", \"however\", \"even though\", \"although\", \"while\", \";\"]\n",
    "split_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, split_tokens)) + r\")\\b|;\")\n",
    "\n",
    "keywords = set(map(str.lower, [\n",
    "    # Panel A1\n",
    "    \"inflation expectation\", \"interest rate\", \"bank rate\", \"fund rate\", \"price\", \n",
    "    \"economic activity\", \"inflation\", \"employment\",\n",
    "    # Panel A2\n",
    "    \"anchor\", \"cut\", \"subdue\", \"decline\", \"decrease\", \"reduce\", \"low\", \"drop\", \"fall\",\n",
    "    \"fell\", \"decelerate\", \"slow\", \"pause\", \"pausing\", \"stable\", \"non-accelerating\", \n",
    "    \"downward\", \"tighten\",\n",
    "    # Panel B1\n",
    "    \"unemployment\", \"growth\", \"exchange rate\", \"productivity\", \"deficit\", \"demand\",\n",
    "    \"job market\", \"monetary policy\",\n",
    "    # Panel B2\n",
    "    \"ease\", \"easing\", \"rise\", \"rising\", \"increase\", \"expand\", \"improve\", \"strong\", \n",
    "    \"upward\", \"raise\", \"high\", \"rapid\"\n",
    "]))\n",
    "\n",
    "junk_phrases = [\n",
    "        \"cookie\", \"cookies\", \"terms of use\", \"privacy policy\", \"ads and content\", \n",
    "        \"by using this site\", \"subscribe\", \"sign up\", \"CNBC\", \"NBCUniversal\", \"copyright\",\n",
    "        \"click\", \"browser\", \"advertise with us\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e1e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Read two json files ===\n",
    "with open(\"./fomc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    fomc_data = json.load(f)\n",
    "\n",
    "with open(\"./cnbc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cnbc_data = json.load(f)\n",
    "    \n",
    "all_data = fomc_data + cnbc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56154a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 10 documents with filtered sentences to filtered_sentences_by_url.json\n"
     ]
    }
   ],
   "source": [
    "# === Result: sentences grouped by URL\n",
    "filtered_sentences_by_url = {}\n",
    "\n",
    "# === Process each item ===\n",
    "for item in all_data:\n",
    "    content = item.get(\"content\", \"\")\n",
    "    url = item.get(\"url\", \"unknown_source\")\n",
    "    source_type = item.get(\"type\", \"unknown_type\")\n",
    "\n",
    "    if not content.strip():\n",
    "        continue\n",
    "\n",
    "    # --- Split into sentences ---\n",
    "    sentences = sentence_pattern.split(content)\n",
    "\n",
    "    valid_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # --- Split further on split tokens ---\n",
    "        parts = split_pattern.split(sentence)\n",
    "        parts = [part.strip() for part in parts if part and not re.match(split_pattern, part)]\n",
    "\n",
    "        for part in parts:\n",
    "            if len(part.split()) < 3 or part.count('\\n') > 3 or len(re.findall(r'[.!?]', part)) < 1:\n",
    "                continue\n",
    "\n",
    "            part_lower = part.lower()\n",
    "\n",
    "            if any(junk_phrase in part_lower for junk_phrase in junk_phrases):\n",
    "                continue\n",
    "\n",
    "            if any(re.search(rf\"\\b{re.escape(keyword)}\\b\", part_lower) for keyword in keywords):\n",
    "                valid_sentences.append(part)\n",
    "\n",
    "    # === If this item had any valid sentences, save them by URL\n",
    "    if valid_sentences:\n",
    "        filtered_sentences_by_url[url] = {\n",
    "            \"type\": source_type,\n",
    "            \"sentences\": valid_sentences\n",
    "        }\n",
    "\n",
    "# === Save\n",
    "output_path = \"filtered_sentences_by_url.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_sentences_by_url, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(filtered_sentences_by_url)} documents with filtered sentences to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d730406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 66 valid sentences.\n",
      "- EDT\n",
      "Share\n",
      "Although swings in net exports have affected the data, recent indicators suggest that economic activity has continued to expand at a solid pace.\n",
      "- The unemployment rate remains low, and labor market conditions remain solid.\n",
      "- Inflation remains somewhat elevated.\n",
      "The Committee seeks to achieve maximum employment and inflation at the rate of 2 percent over the longer run.\n",
      "- The Committee is strongly committed to supporting maximum employment and returning inflation to its 2 percent objective.\n",
      "In assessing the appropriate stance of monetary policy, the Committee will continue to monitor the implications of incoming information for the economic outlook.\n",
      "- The Committee would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that could impede the attainment of the Committee's goals.\n",
      "- The Committee's assessments will take into account a wide range of information, including readings on labor market conditions, inflation pressures and inflation expectations, and financial and international developments.\n",
      "Voting for the monetary policy action were Jerome H.\n",
      "- Federal Reserve on Wednesday kept interest rates in a range between 4.25%-4.5% and kept two cuts in 2025 on the table.\n",
      "Inflation in the U.S., measured by the personal consumption expenditures price index, will rise beyond 3% in 2025, according to an updated Fed forecast.\n",
      "U.S.\n",
      "- Europe’s Stoxx 600 index fell.\n",
      "U.S President Donald Trump insisted he had not yet decided whether to order a U.S.\n",
      "- Federal Reserve Chair Jerome Powell’spost-meeting press conference, the topic of tariffs — specifically, their impact on prices — was a recurring one.\n",
      "“Everyone that I know is forecasting a meaningful increase in inflation in coming months from tariffs because someone has to pay for the tariffs,” Powell said.\n",
      "- “And some of it will fall on the end consumer.”\n",
      "Granted, recent economic data has been upbeat, suggesting the U.S.\n",
      "- economy has been able to — and could still — escape from tariffs mostly unscathed.\n",
      "In May, a better-than-expected139,000 jobs were addedand the unemployment rate was unchanged at 4.2%.Consumer sentiment in early Junewas much more optimistic than forecast, according to a University of Michigan survey.\n",
      "- And, most crucially,inflation in May— based on the consumer price index — ticked up just 0.1% for the month, lower than estimated.\n",
      "But that string of positive data might have to thank the slow process by which tariffs move through the economy.\n",
      "“It takes some time for tariffs to work their way through the chain of distribution to the end consumer.\n",
      "- economy weakening, Powell acknowledged growth will slow “eventually.” In other words, stagflation — the toxic mix of higher prices and slower growth — could be a possibility in the months ahead.\n",
      "The song “I Got Summer on My Mind” went viral in 2022.\n",
      "- Earlier Wednesday, President Donald Trump said the fed funds rate should be at least 2 percentage points lower, and again slammed Chair Jerome Powell,calling him “stupid.”\n",
      "Fed hikes inflation and lowers growth forecastsInflation in the U.S., measured by the personal consumption expenditures price index, will rise beyond 3% in 2025, according to an updated Fed forecast.\n",
      "- The Fed also sees economic growth slowing to 1.4% this year, down from an earlier estimate of 1.7%.\n",
      "- annual inflation coming in at anexpected 3.4% in May.\n",
      "Trump says he hasn’t decided on Iran strikesFor the second time in two days, Trump on Wednesdaymet his national security team in the White Houseamid the Israel-Iran conflict.\n",
      "- Ambassador to Israel Mike Huckabee said evacuation flights and cruise ship departures were being arranged for American citizens seeking to leave Israel.\n",
      "[PRO]‘Profound impact’ on oil market: JPMorganThe current jump in oil prices because of the simmering conflict in the Middle East might not lead to a long-term price shock, according to historical data analyzed byJPMorgan.\n",
      "- and European Union are running out of time to strike a deal on trade tariffs — and analysts say several key sticking points could make an agreement impossible.\n",
      "Negotiations have been slow since both the U.S.\n",
      "- and EU temporarily cut duties on each other until July 9.\n",
      "- Here’s how Wall Street is reacting to the Fed’s updated rate cut outlook\n",
      "Summary:\n",
      "Pre-Markets\n",
      "U.S.\n"
     ]
    }
   ],
   "source": [
    "# === Done ===\n",
    "filtered_sentences = []\n",
    "\n",
    "for url, data in filtered_sentences_by_url.items():\n",
    "    for sentence in data[\"sentences\"]:\n",
    "        filtered_sentences.append(sentence)\n",
    "    \n",
    "print(f\"✅ Found {len(filtered_sentences)} valid sentences.\")\n",
    "for s in filtered_sentences[:20]:  # preview first 20\n",
    "    print(\"-\", s)\n",
    "\n",
    "# === Save to json ===\n",
    "with open(\"processed_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_sentences, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5ffb7",
   "metadata": {},
   "source": [
    "## Loading roBERTa base model and Mistral model for classification and summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecc9bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Gemma on device: cuda\n",
      "✅ Loaded finetuned RoBERTa on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Load Mistral model (causal LM)\n",
    "save_dir = \"../models/saved_gemma_2_2b_it_model\"\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(save_dir, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# Move to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma_model = gemma_model.to(device)\n",
    "\n",
    "print(f\"✅ Loaded Gemma on device: {device}\")\n",
    "\n",
    "# === Load finetuned RoBERTa (sequence classification)\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"../models/finetuned_roberta_model_pre_overfit_epoch_8\")\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"../models/finetuned_roberta_model_pre_overfit_epoch_8\")\n",
    "\n",
    "# Move to CUDA if available\n",
    "roberta_model = roberta_model.to(device)\n",
    "\n",
    "print(f\"✅ Loaded finetuned RoBERTa on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deee2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./processed_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    processed_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370ceb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:14<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 66 sentence labels to roberta_sentence_labels.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Process sentences ===\n",
    "results = []\n",
    "\n",
    "for sentence in tqdm(processed_sentences):\n",
    "    inputs = roberta_tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = logits.argmax(dim=-1).item()\n",
    "\n",
    "    # OPTIONAL: map label to text (depends on your training labels!)\n",
    "    label_map = {\n",
    "        0: \"dovish\",\n",
    "        1: \"neutral\",\n",
    "        2: \"hawkish\"\n",
    "    }\n",
    "    label_text = label_map.get(predicted_class_id, str(predicted_class_id))\n",
    "\n",
    "    results.append({\n",
    "        \"sentence\": sentence,\n",
    "        \"label_id\": predicted_class_id,\n",
    "        \"label\": label_text\n",
    "    })\n",
    "\n",
    "# === Save to JSON ===\n",
    "output_path = \"roberta_sentence_labels.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(results)} sentence labels to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320b34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
